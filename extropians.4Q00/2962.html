<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
                      "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<title>extropians: Re: The mathematics of effective perfection</title>
<meta name="Author" content="Eliezer S. Yudkowsky (sentience@pobox.com)">
<meta name="Subject" content="Re: The mathematics of effective perfection">
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Re: The mathematics of effective perfection</h1>
<!-- received="Sun Nov 26 15:34:52 2000" -->
<!-- isoreceived="20001126223452" -->
<!-- sent="Sun, 26 Nov 2000 17:35:27 -0500" -->
<!-- isosent="20001126223527" -->
<!-- name="Eliezer S. Yudkowsky" -->
<!-- email="sentience@pobox.com" -->
<!-- subject="Re: The mathematics of effective perfection" -->
<!-- id="3A21902F.C33F907E@pobox.com" -->
<!-- inreplyto="NDBBIBGFAPPPBODIPJMMCEIPENAA.ben@intelligenesis.net" -->
<strong>From:</strong> Eliezer S. Yudkowsky (<a href="mailto:sentience@pobox.com?Subject=Re:%20The%20mathematics%20of%20effective%20perfection&In-Reply-To=&lt;3A21902F.C33F907E@pobox.com&gt;"><em>sentience@pobox.com</em></a>)<br>
<strong>Date:</strong> Sun Nov 26 2000 - 15:35:27 MST
<p>
<!-- next="start" -->
<ul>
<li><strong>Next message:</strong> <a href="2963.html">slash@extropy.org: "24 Hours of Extropy Institute Portal Headlines For Extropy Mailing List"</a>
<li><strong>Previous message:</strong> <a href="2961.html">Robert Coyote: "Re: Two great articles on ignoring government"</a>
<li><strong>In reply to:</strong> <a href="2954.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2965.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<li><strong>Reply:</strong> <a href="2965.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2962">[ date ]</a>
<a href="index.html#2962">[ thread ]</a>
<a href="subject.html#2962">[ subject ]</a>
<a href="author.html#2962">[ author ]</a>
</ul>
<hr noshade><p>
<!-- body="start" -->
<p>
Ben Goertzel wrote:
<br>
<em>&gt; 
</em><br>
<em>&gt; First of all, you certainly have not shut the door to a mathematical proof
</em><br>
<em>&gt; of transhuman
</em><br>
<em>&gt; minds' significant imperfection.  The analogy to thermodynamics is OK, but
</em><br>
<em>&gt; it doesn't give you
</em><br>
<em>&gt; guidance as to what the actual probabilities involved are.  You just pull
</em><br>
<em>&gt; the probability values
</em><br>
<em>&gt; out of a hat (leading to your figures of decillion years, centillion years,
</em><br>
<em>&gt; etc.), by analogy to
</em><br>
<em>&gt; the logic of much simpler systems.
</em><br>
<p>Well, sure.  Once you've said that the expiration date on a description
<br>
increases exponentially with RAM, or that the time for a thermodynamic
<br>
miracle increases exponentially with the number of particles in the
<br>
system, you've said everything there is to say from a complexity-theory
<br>
perspective.  Trying to come up with real numbers for that would be
<br>
totally hopeless.
<br>
<p><em>&gt; The door is open for a real mathematical
</em><br>
<em>&gt; analysis of
</em><br>
<em>&gt; the lossiness and error-prone-ness of knowledge and inference in minds of
</em><br>
<em>&gt; various sizes.
</em><br>
<p>I don't see how you can possibly do this.  Given a specific physical
<br>
system, you can estimate the error rate of the underlying processes and
<br>
demonstrate that, e.g., there is a 10^-50 chance of a single error
<br>
occurring in ten thousand years of operation.  I don't see how you could
<br>
possibly get specific numbers for software errors in a program as complex
<br>
as Webmind, much less a human, much less a totally unspecified transhuman.
<br>
<p><em>&gt; However,
</em><br>
<em>&gt; I do not intend to provide this at the moment -- though I do hope, before I
</em><br>
<em>&gt; die (if I ever do, which
</em><br>
<em>&gt; I hope I don't ;), to create a real mathematical theory of mind that would
</em><br>
<em>&gt; allow such questions
</em><br>
<em>&gt; be be explored...
</em><br>
<p>There will never be a real mathematical theory of mind that is less
<br>
complex than a mind itself; no useful part of the mind will ever be
<br>
describable except by all the individual equations governing
<br>
transistor-equivalents or neurons.  The wish for such a theory is simply
<br>
physics envy.  We live in a world where physical systems turn out to
<br>
exhibit all sorts of interesting, mathematically describable high-level
<br>
behaviors; but neither the biology of evolved organisms, nor the behavior
<br>
of evolved minds, nor the computer programs we design, exhibit any such
<br>
tendency.  If you took the sum of all the numbers in a computer's RAM and
<br>
plotted it over time, you might find that it danced an airy Gaussian
<br>
minuet around a mean, but I don't think you will ever find any behavior
<br>
more interesting than that - there is no reason why such a behavior would
<br>
exist and no precedent for expecting one.  Mathematics is an
<br>
extraordinarily powerful tool which will never be useful to cognitive
<br>
scientists.  We'll just have to live with that.
<br>
&nbsp;
<br>
<em>&gt; Second, your investigation raises an interesting question.  If a human were
</em><br>
<em>&gt; faced with the tasks of a
</em><br>
<em>&gt; leaf-cutter ant -- find some leaves to eat, carry them back home and store
</em><br>
<em>&gt; them, then eat them when hungry --
</em><br>
<em>&gt; then presumably the human would not make very many errors of fact or
</em><br>
<em>&gt; judgment.  (If the human went insane
</em><br>
<em>&gt; it would be from boredom ;)
</em><br>
<p>Precisely.  Effective perfection is achievable.
<br>
<p><em>&gt; The point is: We have sought out tasks that strain our cognitive
</em><br>
<em>&gt; architecture.
</em><br>
<p>Yes.  And, at our current level of intelligence, staying sane - or rather,
<br>
developing a personal philosophy and learning self-knowledge - strains
<br>
these abilities to the limit.  Indeed, before the development of
<br>
evolutionary psychology and related sciences, humanity's philosophers
<br>
totally failed to deduce these facts through introspection - even though
<br>
all the information was, theoretically, available.
<br>
<p>We have no ability to observe individual neurons.
<br>
<p>In the realm of the mind, we have no ability to construct tools, or tools
<br>
to build tools, whereby we could examine intermediate levels.
<br>
<p>We have no direct access to, or manipulation of, our underlying functional
<br>
layers.
<br>
<p>Of course we fail.
<br>
<p>Now, you can make up all kinds of reasons why AIs or transhumans might run
<br>
into minor difficulties decoding neural nets, failing to achieve complete
<br>
perfection, and so on.  But it looks to me like simple common sense says
<br>
that, if we humans had all these abilities, we would have achieved vastly
<br>
more than we have now.  No, we still might not be perfect.  But we would,
<br>
at the very least, be vastly better.  And, given enough time, or given
<br>
transhuman intelligence, we might well achieve effective perfection in the
<br>
domain of sanity.
<br>
<p>Will transhumans seek out tasks that strain their abilities?  I don't
<br>
know.  Will they take these domains and make them part of themselves,
<br>
assimilating them, running mental simulations, so that their self is also
<br>
a strain to understand?  Maybe - the border between external and internal
<br>
reality gets a bit blurred when you can *really* run mental simulations. 
<br>
Even so, however, the domain of &quot;sanity&quot; is only a subset of the domain of
<br>
&quot;self-observation&quot;.  I think that the problem of sanity can be solved,
<br>
completely and forever.  I think it's a problem that strains our current
<br>
minds and current access, but would not strain either a transhuman mind,
<br>
or a mind with the ability to access the neural-analogue level and build
<br>
tools to build tools.
<br>
<p><em>&gt; I don't claim to have demonstrated anything here -- except that there is a
</em><br>
<em>&gt; lot of room for doubt where these
</em><br>
<em>&gt; matters are concerned... that the apparent solidity of arguments based on
</em><br>
<em>&gt; the thermodynamic analogy is only
</em><br>
<em>&gt; apparent.  You may well be right Eliezer, but by ignoring &quot;cognitive
</em><br>
<em>&gt; science&quot; issues you really ignore the
</em><br>
<em>&gt; crux of it all.  Of course our knowledge of transhuman psychology is fairly
</em><br>
<em>&gt; limited, which just means, to my
</em><br>
<em>&gt; mind, that a lot of humility is appropriate in the face of these huge
</em><br>
<em>&gt; questions.
</em><br>
<p>I never wanted to ignore &quot;cognitive science&quot; issues.  The whole point of
<br>
the post was to take the ball out of the mathematical court and drop-kick
<br>
it back into the cognitive one.
<br>
<p><em>&gt; With a project like building a thinking machine, or proving a theorem, or
</em><br>
<em>&gt; even composing a piece of music,
</em><br>
<em>&gt; one's ideas can eventually be refuted by experience.  The algorithm fails,
</em><br>
<em>&gt; the proof fails, the composition
</em><br>
<em>&gt; sounds bad [to oneself, or to others].  In noodling about transhuman
</em><br>
<em>&gt; psychology, there's no feedback from
</em><br>
<em>&gt; external physical or social or mathemtaical reality, so anything goes --
</em><br>
<em>&gt; until 500 years from now when AI's
</em><br>
<em>&gt; look back and laugh at our silly ideas...
</em><br>
<p>Of course.  But, though perhaps I am mistaken, it looks to me like your
<br>
beliefs about transhumanity have effects on what you do in the
<br>
here-and-now.  Certainly your projections about transhumanity, and your
<br>
actions in the present, spring from the same set of underlying
<br>
assumptions.
<br>
<p>--              --              --              --              -- 
<br>
Eliezer S. Yudkowsky                          <a href="http://singinst.org/">http://singinst.org/</a> 
<br>
Research Fellow, Singularity Institute for Artificial Intelligence
<br>
<p><!-- body="end" -->
<hr noshade>
<ul>
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="2963.html">slash@extropy.org: "24 Hours of Extropy Institute Portal Headlines For Extropy Mailing List"</a>
<li><strong>Previous message:</strong> <a href="2961.html">Robert Coyote: "Re: Two great articles on ignoring government"</a>
<li><strong>In reply to:</strong> <a href="2954.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="2965.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<li><strong>Reply:</strong> <a href="2965.html">Ben Goertzel: "RE: The mathematics of effective perfection"</a>
<!-- reply="end" -->
<li><strong>Messages sorted by:</strong> 
<a href="date.html#2962">[ date ]</a>
<a href="index.html#2962">[ thread ]</a>
<a href="subject.html#2962">[ subject ]</a>
<a href="author.html#2962">[ author ]</a>
</ul>
<!-- trailer="footer" -->
<hr noshade>
<p>
<small>
<em>
This archive was generated by <a href="http://www.hypermail.org/">hypermail 2b30</a> 
: <em>Mon May 28 2001 - 09:50:31 MDT</em>
</em>
</small>
</body>
</html>
